{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb11506",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR=\"/home/ec2-user/SageMaker\"\n",
    "SAGEMAKER_DIR = \"/home/ec2-user/SageMaker/neural_nets_memorization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491db48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "CPU times: user 102 ms, sys: 10.5 ms, total: 112 ms\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%capture \n",
    "#os.path.join(SAGEMAKER_DIR,\"neural_nets_memorization\")\n",
    "!git pull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d57a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.96 s, sys: 2.58 s, total: 5.53 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "! pip install --upgrade {SAGEMAKER_DIR}\n",
    "#!pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187b7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_memorization import params\n",
    "from neural_nets_memorization import utils\n",
    "from neural_nets_memorization import dataset_management\n",
    "from neural_nets_memorization import unbalanced_datasets\n",
    "from neural_nets_memorization import privacy\n",
    "from neural_nets_memorization import model_training\n",
    "from neural_nets_memorization import models\n",
    "from neural_nets_memorization import pate\n",
    "from neural_nets_memorization import dp_prototypicalness\n",
    "from neural_nets_memorization import memorization_estimator\n",
    "\n",
    "import os \n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import numba\n",
    "from numba import jit, njit, prange, bool_, int_\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f9316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inspect.getsource(model_training.train_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05075e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainset, testset), (trainloader,\n",
    "                      testloader) = dataset_management.load_MNIST_datasets_and_loaders()# load_CIFAR10_datasets_and_loaders()\n",
    "num_classes =10\n",
    "device = params.get_default_device()\n",
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50ed068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon - 400.0\n",
      "Using sigma=0.15641113281250002 and C=0.3\n",
      "new data loader created <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Training epoch: 0/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  151.865117931495\n",
      "Epoch [0], \n",
      "\tAccuracy: approx_train_accuracy: 0.7977, train_accuracy: 0.9404, val_accuracy: 0.9478\n",
      "\tLoss: approx_train_loss: 0.9722, train_loss: 0.3505, val_loss: 0.2940\n",
      "time for epoch 0 : 0:00:39.086671\n",
      "Training epoch: 1/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  193.31954708080784\n",
      "Epoch [1], \n",
      "\tAccuracy: approx_train_accuracy: 0.8691, train_accuracy: 0.9520, val_accuracy: 0.9542\n",
      "\tLoss: approx_train_loss: 0.5693, train_loss: 0.2811, val_loss: 0.2505\n",
      "time for epoch 1 : 0:00:38.812214\n",
      "Training epoch: 2/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  232.8935691346879\n",
      "Epoch [2], \n",
      "\tAccuracy: approx_train_accuracy: 0.8744, train_accuracy: 0.9512, val_accuracy: 0.9555\n",
      "\tLoss: approx_train_loss: 0.5534, train_loss: 0.2738, val_loss: 0.2456\n",
      "time for epoch 2 : 0:00:38.461610\n",
      "Training epoch: 3/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  272.98042948732234\n",
      "Epoch [3], \n",
      "\tAccuracy: approx_train_accuracy: 0.8762, train_accuracy: 0.9524, val_accuracy: 0.9541\n",
      "\tLoss: approx_train_loss: 0.5531, train_loss: 0.2928, val_loss: 0.2692\n",
      "time for epoch 3 : 0:00:38.466695\n",
      "Training epoch: 4/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  313.0672898399568\n",
      "Epoch [4], \n",
      "\tAccuracy: approx_train_accuracy: 0.8791, train_accuracy: 0.9557, val_accuracy: 0.9599\n",
      "\tLoss: approx_train_loss: 0.5494, train_loss: 0.2652, val_loss: 0.2281\n",
      "time for epoch 4 : 0:00:39.177878\n",
      "Training epoch: 5/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  354.26529983989246\n",
      "Epoch [5], \n",
      "\tAccuracy: approx_train_accuracy: 0.8756, train_accuracy: 0.9554, val_accuracy: 0.9564\n",
      "\tLoss: approx_train_loss: 0.5395, train_loss: 0.2520, val_loss: 0.2275\n",
      "time for epoch 5 : 0:00:38.615885\n",
      "Training epoch: 6/7\n",
      "privacy used  41.943469841947866\n",
      "privacy used  395.1214176406585\n",
      "Epoch [6], \n",
      "\tAccuracy: approx_train_accuracy: 0.8818, train_accuracy: 0.9538, val_accuracy: 0.9593\n",
      "\tLoss: approx_train_loss: 0.5512, train_loss: 0.2678, val_loss: 0.2211\n",
      "time for epoch 6 : 0:00:38.897588\n",
      "total time: 0:04:32.053053\n"
     ]
    }
   ],
   "source": [
    "eps = 400.\n",
    "# 40 was 94 \n",
    "a = model_training.train_model(\n",
    "    model= models.CNN_model_factory(),\n",
    "    model_name=\"test\",\n",
    "    epochs=epochs,\n",
    "    train_loader= trainloader,\n",
    "    device = device,\n",
    "    test_loader=testloader,  # can be None - only called in per_epoch_callback hook\n",
    "    max_grad_norm=params.MAX_GRAD_NORM,\n",
    "    tensorboard_path=\".\",\n",
    "    criterion = params.criterion,\n",
    "    eps=eps,\n",
    "    delta=1e-5,#None,\n",
    "    per_epoch_callbacks=model_training.log_everything_callbacks,\n",
    "    save_path=None,\n",
    "    overwrite=False, verbose= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34039253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo : print eps\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/ec2-user/SageMaker/neural_nets_memorization/notebooks/\n",
    "\n",
    "# go to https://neural-nets-gpu-instance.notebook.us-east-1.sagemaker.aws/proxy/6006/#scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2.\n",
    "model_ =models.CNN_model_factory()\n",
    "results, model_predictions = memorization_estimator.run_single_experiment(\n",
    "                          model = model_,\n",
    "                          dataloader = trainloader,\n",
    "                          testloader=testloader,\n",
    "                          memorization_dataloader = trainloader,\n",
    "                          save_path =None ,\n",
    "                          tensorboard_path=\".\",\n",
    "                          model_name=\"CNN\",\n",
    "                          epochs=epochs,\n",
    "                          device=params.get_default_device(),\n",
    "                          N=60000,\n",
    "                          epsilon=eps,\n",
    "                          overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ =models.CNN_model_factory()\n",
    "results, model_predictions = memorization_estimator.run_single_experiment(\n",
    "                          model = model_,\n",
    "                          dataloader = trainloader,\n",
    "                          testloader=testloader,\n",
    "                          memorization_dataloader = trainloader,\n",
    "                          save_path =None ,\n",
    "                          tensorboard_path=\".\",\n",
    "                          model_name=\"CNN\",\n",
    "                          epochs=epochs,\n",
    "                          device=params.get_default_device(),\n",
    "                          N=60000,\n",
    "                          epsilon=5.0,\n",
    "                          overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1da171",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60000\n",
    "m_ratio = 0.8\n",
    "trials = 2000\n",
    "\n",
    "mem_exp_masks = memorization_estimator.set_up_memorization_experiment_dataset_masks(\n",
    "        filepath=None, N=N, m_ratio=m_ratio, trials=trials, overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba903659",
   "metadata": {},
   "outputs": [],
   "source": [
    "memorization_estimator.run_all_experiments(mem_experiment_masks=mem_exp_masks,\n",
    "                        base_save = \".\",\n",
    "                        predictions_filepath=\"predictions.txt\",\n",
    "                        memorization_experiments_meta_filepath=\"ignore.txt\",\n",
    "                        dataset = trainset,\n",
    "                        memorization_dataloader = trainloader,\n",
    "                        epochs=5,\n",
    "                        model_factory=models.CNN_model_factory,\n",
    "                        device=params.get_default_device(),\n",
    "                        predictions=None,\n",
    "                        memorization_experiments_meta=None,\n",
    "                        epsilon=3.,\n",
    "                        max_grad_norm=params.MAX_GRAD_NORM,\n",
    "                        overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training.get_predictions(model= model_, data_loader= trainloader, device = params.get_default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_dp =models.CNN_model_factory()\n",
    "\n",
    "results, model_predictions = memorization_estimator.run_single_experiment(\n",
    "                          model = model_no_dp,\n",
    "                          dataloader = trainloader,\n",
    "                          testloader=None,\n",
    "                          memorization_dataloader = trainloader,\n",
    "                          save_path =None ,\n",
    "                          tensorboard_path=None,\n",
    "                          model_name=\"CNN\",\n",
    "                          epochs=5,\n",
    "                          device=params.get_default_device(),\n",
    "                          N=60000,\n",
    "                          epsilon=None,\n",
    "                          overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61218ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "model = models.CNN_model_factory()\n",
    "\n",
    "\n",
    "privacy_engine, model, optimizer, data_loader = privacy.privatize(model=model, \n",
    "                                                                  data_loader=trainloader, \n",
    "                                                                  epochs=10, epsilon= 1., delta=1.e-5, max_grad_norm=1., device= device)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with BatchMemoryManager(data_loader=trainloader, max_physical_batch_size=2, optimizer=optimizer) as new_data_loader:\n",
    "    for data, label in new_data_loader:\n",
    "        print(data)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookie here # https://jovian.ai/roubish/final-course-assignment\n",
    "def train_model(\n",
    "        *,\n",
    "        model,\n",
    "        model_name=\"\",\n",
    "        epochs,\n",
    "        train_loader,\n",
    "        device,\n",
    "        test_loader=None,  # can be None - only called in per_epoch_callback hook\n",
    "        max_grad_norm=params.MAX_GRAD_NORM,\n",
    "        tensorboard_path=None,\n",
    "        criterion,\n",
    "        eps=None,\n",
    "        delta=None,\n",
    "        per_epoch_callbacks=None,\n",
    "        save_path=None,\n",
    "        overwrite=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    # set up\n",
    "    first_start = datetime.datetime.now()\n",
    "    estimated_total_correct = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    ret = {\n",
    "        \"save_path\": save_path,\n",
    "        \"privacy\": {\n",
    "            \"epsilon\": eps,\n",
    "            \"delta\": delta\n",
    "        },\n",
    "        \"history\": history,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    print(f\"epsilon - {eps}\")\n",
    "    if save_path is not None and os.path.exists(save_path) and not overwrite:\n",
    "        print(f\"path {save_path} already saved to. SKIPPING.\")\n",
    "        return ret\n",
    "    writer = SummaryWriter(\n",
    "        tensorboard_path) if tensorboard_path is not None else None\n",
    "\n",
    "    per_epoch_callbacks = per_epoch_callbacks or []\n",
    "    # set up privacy experiment if needed (use memory safe data loader as needed)\n",
    "    dataloader = train_loader\n",
    "    optimizer = None\n",
    "    if eps is not None:\n",
    "        \n",
    "        _, model, optimizer, train_loader = privacy.privatize(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            epochs=epochs,\n",
    "            epsilon=eps,\n",
    "            delta=delta,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            device=device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(optimizer)\n",
    "    print(\"do we exist?\")\n",
    "    optimizer = optimizer or utils.get_new_optimizer(model) \n",
    "\n",
    "    # create writer\n",
    "\n",
    "    def _train(dataloader):\n",
    "        \"\"\" Helper to train either with DP or not (creates closure)\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            start = datetime.datetime.now()\n",
    "            print(f\"Training epoch: {epoch}/{epochs}\")\n",
    "            estimated_total_correct, estimated_epoch_train_losses = model_training.train_single_epoch(\n",
    "                model=model,\n",
    "                train_loader=dataloader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                device=device)\n",
    "\n",
    "            result = {\n",
    "                \"epoch\":\n",
    "                epoch,\n",
    "                \"approx_train_loss\":\n",
    "                np.mean(estimated_epoch_train_losses),\n",
    "                \"approx_train_accuracy\":\n",
    "                estimated_total_correct / len(dataloader.dataset)\n",
    "            }  # todo : do something more with result\n",
    "\n",
    "            # call all callbacks\n",
    "\n",
    "            for callback in per_epoch_callbacks:  # to do : check if callback is called on the last batch\n",
    "                callback(model=model,\n",
    "                         epoch=epoch,\n",
    "                         train_loader=dataloader,\n",
    "                         test_loader=test_loader,\n",
    "                         optimizer=optimizer,\n",
    "                         criterion=criterion,\n",
    "                         result=result,\n",
    "                         writer=writer,\n",
    "                         device=device)\n",
    "            history.append(result)\n",
    "            epoch_end(result=result)\n",
    "            print(\n",
    "                f\"time for epoch {epoch} : {(datetime.datetime.now() - start)}\"\n",
    "            )\n",
    "\n",
    "    # train with DP\n",
    "    if eps is None:\n",
    "        # train without DP\n",
    "        _train(dataloader)\n",
    "    else:\n",
    "        with BatchMemoryManager(\n",
    "                data_loader=train_loader,\n",
    "                max_physical_batch_size=model_training.MAX_PHYSICAL_BATCH_SIZE,\n",
    "                optimizer=optimizer) as new_data_loader:\n",
    "            print(f\"new data loader created {type(new_data_loader)}\")\n",
    "            _train(new_data_loader)\n",
    "\n",
    "    if save_path is not None:\n",
    "        utils.save_model(model, save_path)\n",
    "\n",
    "    print(f\"total time: {(datetime.datetime.now() - first_start)}\")\n",
    "    if writer is not None:\n",
    "        writer.flush()\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_model(\n",
    "        model= models.CNN_model_factory(),\n",
    "        model_name=\"test\",\n",
    "        epochs=4,\n",
    "        train_loader= trainloader,\n",
    "        device = device,\n",
    "        test_loader=None,  # can be None - only called in per_epoch_callback hook\n",
    "        max_grad_norm=params.MAX_GRAD_NORM,\n",
    "        tensorboard_path=None,\n",
    "        criterion = params.criterion,\n",
    "        eps=4.,\n",
    "        delta=1e-5,#None,\n",
    "        per_epoch_callbacks=None,\n",
    "        save_path=None,\n",
    "        overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2961ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_indices(dataset, class_ordering, class_distribution_counts,\n",
    "                         class_to_dataset):\n",
    "    \"\"\"\n",
    "    class_ordering : a list continaing numbers 0-9 (inclusive), in some order\n",
    "    distribution_counts : a list denoting the number of points to sample from a particular class\n",
    "    class_to_dataset : a dictionary mapping class-id to a dataset of only data from that class\n",
    "\n",
    "    returns a dictionary of lists of indices according to class_ordering and class_distribution_counts\n",
    "    \"\"\"\n",
    "    indices = {}\n",
    "\n",
    "    for i, class_id in enumerate(class_ordering):\n",
    "        og_class_dataset = class_to_dataset[class_id]\n",
    "        og_class_indices = class_to_dataset[class_id].indices.T.numpy(\n",
    "        ).nonzero()[0]\n",
    "        #og_class_dataset.indices.nonzeros()\n",
    "\n",
    "        number_of_points_from_class = int(\n",
    "            min(class_distribution_counts[i], len(og_class_dataset)))\n",
    "        # randomly pick a subset of the data points of a particular class\n",
    "        indices[class_id] = np.random.choice(og_class_indices,\n",
    "                                             size=number_of_points_from_class,\n",
    "                                             replace=False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_datasets =num_teachers = 30\n",
    "\n",
    "class_ordering = np.arange(len(trainset.classes))\n",
    "\n",
    "        \n",
    "class_to_dataset = unbalanced_datasets.get_class_to_dataset(trainset)\n",
    "class_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c =0\n",
    "for i in class_to_dataset[0].dataset:\n",
    "    c+=1\n",
    "#(dataset.targets[dataset.targets==class_id],dataset.data[dataset.targets==class_id] )\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = class_to_dataset[0]\n",
    "ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529edd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_dataset = unbalanced_datasets.get_class_to_dataset(trainset)\n",
    "class_distribution_counts = unbalanced_datasets.get_class_distribution_counts_from_dataset(trainset)\n",
    "\n",
    "weighted_indices = get_weighted_indices(trainset, class_ordering, class_distribution_counts, class_to_dataset)\n",
    "\n",
    "plt.plot(np.arange(len(class_distribution_counts)),\n",
    "             class_distribution_counts)\n",
    "plt.title(\"class sizes of the weighted datasets\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35ab72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1346467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
